\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2018}
\usepackage{amsmath,graphicx}
\usepackage{epsfig,epstopdf,subfigure,slashbox}
\usepackage{amsfonts}
\usepackage{datetime,algorithm,algorithmic,multirow}
\usepackage{xcolor,color}
\usepackage{cite}
\usepackage{amsthm}
\usepackage{array}
\DeclareMathOperator*{\argmax}{argmax}
\pagestyle{empty}

\tolerance=1
\emergencystretch=\maxdimen
%\hyphenpenalty=1000
%\hbadness=1000
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}


% Title.
% ------
%\title{Speech Emotion Recognition Considering Emotion Distribution Information in Dimensional Emotion Space with Emotion-Pair based Framework }
\title{Emotion Recognition from Variable-Length Speech Segments Using Deep Learning on Spectrograms}
% Single address.
% ---------------
\name{Xi Ma$^{1,3}$, Zhiyong Wu$^{1,2,3}$, Jia Jia$^{1,3}$, Mingxing Xu$^{1,3}$, Helen Meng$^{1,2}$, Lianhong Cai$^{1,3}$}

\address{$^1$Tsinghua-CUHK Joint Research Center for Media Sciences, Technologies and Systems,\\
Graduate School at Shenzhen, Tsinghua University, Shenzhen 518055, China\\
  $^2$Department of Systems Engineering and Engineering Management,\\
  The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China\\
  $^3$Tsinghua National Laboratory for Information Science and Technology (TNList),\\
  Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China}

\email{max15@mails.tsinghua.edu.cn, \{zywu,hmmeng\}@se.cuhk.edu.hk, \{jjia,xumx,clh-dcs\}@tsinghua.edu.cn}
%  {\small \tt max15@mails.tsinghua.edu.cn}, {\small \tt \{zywu,hmmeng\}@se.cuhk.edu.hk}, {\small \tt \{jjia,xumx,clh-dcs\}@tsinghua.edu.cn}}


%\linespread{0.92}
\begin{document}

\maketitle
%
\begin{abstract}
In this work, a approach of emotion recognition is proposed for variable-length speech segments, which applied deep neutral network to spectrograms directly. The spectrogram contains more comprehensive para-lingual information than the traditional acoustic features, so it would be better to be the input for the emotion recognition system. We tried to extract such information from spectrograms and accomplish the emotion recognition task by combining Convolutional
Neural Networks (CNNs) with Recurrent Neural Networks (RNNs). Meanwhile, in order to handle the variable-length speech segments, the neural network structure we constructed can deal with the variable-length input sequence. Compared to split the speech input into the smaller and fixed-length segments, our method can solve the loss of accuracy introduced in the speech segmentation process. We evaluated the emotion recognition model on the IEMOCAP dataset over four emotions. Experimental results demonstrate that the proposed method outperforms the fixed-length neural network on both weighted accuracy (WA) and unweighted accuracy (UA).
\end{abstract}

\noindent\textbf{Index Terms}: Speech Emotion Recognition, Variable-Length Speech Segments, Spectrogram, Deep Neural Network.

\section{Introduction}

Emotion recognition plays an important role in many applications, especially in human-computer interaction systems that are increasingly common today. As one of the main communication media between human beings, voice has attracted wide attentions from researchers~\cite{ayadi2011}. Speech contains a wealth of emotional information. How to extract such information from speech signal is of great importance for automatic speech emotion recognition.

As an important part of speech emotion recognition, the extraction of the most relevant acoustic features has attracted lot of research interests. Most of these researches are devoted to designing some hand-crafted features that is most distinctive for emotion recognition. Recently, however, a trend in the machine learning community has emerged towards deriving a representation of the input signal directly from raw, unprocessed data. The reason behind this idea is that, ultimately, the network learns an intermediate representation of the raw input signal automatically that better suits the task at hand and hence leads to improve performance. Motivated by this, we tried to construct a emotion recognition system by leveraging spectrogram and deep neural network.

A spectrogram is a time-frequency decomposition of a signal that indicates its frequency content over time. Convolutional Neural Networks (CNNs) are first constructed to learn effectively spatial spectrogram patterns that represent the emotional information; Recurrent Neural Network (RNNs) are then used to model the temporal structure across the sentence being represented by the spectrogram; the final emotion categories are derived by a full-connected layer.

The idea of this work is similar to the previous Aharon's work~\cite{lee2011}. However, our neural network possesses the advantages of being able to handle the variable-length speech segments. Compared to split the speech input in the smaller and fixed-length segments, our method can solve the loss of accuracy introduced in the speech segmentation process. In the IEMOCAP dataset~\cite{busso2008}, we can achieve an unweighted accuracy (UA) of 62.54\% using 10-folds (leave-one-speaker out) cross validation, which is a 4.08\% absolute (6.98\% relative) improvement over the fixed-length approach. The weighted accuracy (WA) on the same database is 57.85\%, which also outperforms the fixed-length approach with 1.47\% absolute (2.61\% relative) improvement.

The reset of the paper is organized as follows. Section~\ref{sec:related_work} summarizes the previous related work. The spectrogram based speech emotion recognition framework is then detailed in Section~\ref{sec:recognition_framework}. The proposed variable-length neural network structure is described in Section~\ref{sec:variable_length}. Experiments and results are presented in Section~\ref{sec:expeiments}. Section~\ref{sec:conclusion} concludes the paper.

\section{Related Work}
\label{sec:related_work}

As a common issue for speech emotion recognition, feature extraction aims to design functions that map the raw speech signal to the most relevant representation to the emotions. There have been many studies on feature extraction for speech emotion recognition. In~\cite{busso2009, cowie2001, vayrynen2013}, prosody-based acoustic features, including pitch-related, energy-related and timing features have been widely used for recognizing speech emotion. Spectral-based acoustic features also play important role in emotion recognition, such as Linear Prediction Coefficients (LPC)~\cite{rabiner1978}, Linear Prediction Cepstral Coefficients (LPCC)~\cite{atal1974} and Mel-frequency Cepstral Coefficients (MFCC)~\cite{davis1980}. In~\cite{gobl2003}, voice quality features have also been shown to be related to emotions.

In the recent years, deep learning methodologies and tools have been introduced to speech processing area, used for feature extraction, classification/regression, or both [9-14]. Researchers have shown that replacing hand-crafted low-level (frame-level) features with statistical learning by the different layers of the deep network can significantly enhance the accuracy of classification and regression solutions. In the speech recognition, one of the first studies that suggested learning better features for automatic speech recognition (ASR) that used directly the speech waveform was Jaitly and Hinton [6]. Although they did not train the system in an end-to-end manner, they proposed learning an intermediate representation by training a Restricted Boltzmann Machine directly on the speech time signal. Bhargava and Rose [7] used stacked bottleneck deep neural networks (DNNs) trained on windowed speech waveforms and obtained results only slight worse than corresponding MFCC on the same architecture. Sainath et al. match the performance of a large-vocabulary speech recognition (LVCSR) system based on log-Mel filterbank energies by using a Convolutional, LSTM-DNN [8, 9]. In [20-21], a recently published state of the art robust speech recognition system is described based on linearly-spaced spectrograms. Besides, direct use of Mel-scale spectrograms for speaker recognition was proved successful as well [19].

In the field of speech emotion recognition, several studies have been carried out using deep neural network for feature learning. Recently, George et al. [] proposed a convolutional recurrent neural network that operates on the raw signal, to perform an end-to-end spontaneous emotion prediction task from speech data. Aharon et al. [] also combined CNN with LSTM to classify emotions from linearly-spaced spectrograms, which achieves beyond the state-of-the-art accuracy on the common benchmarking dataset IEMOACP. However, all of these methods split the speech input into the smaller and fixed-length segments, which introduce the loss of accuracy in the training and predicting stage. Our method proposed to use a variable-length neural network to solve this problem.

\section{The Proposed Recognition System}
\label{sec:recognition_system}

In [], they first split each sentence longer than 3 seconds to shorter sub-sentences of equal lengths. The part of no longer than 3 second is padded to 3 second with zero after extracting spectrograms. Each sub-sentence is assigned the emotion labeling of the corresponding whole sentence. These shorter sentences are used throughout the proposed system, where only during the testing phase they evaluate the prediction for the whole sentences by averaging the posterior probabilities of the respective sub-sentences. Although this method can reduce the difficulty for constructing neural network (ensure that the lengths of inputs are equal), some errors will also be introduced. First, it is not a good treatment to assign each sub-sentence to the emotion label of the corresponding whole sentence. The observation indicates that only a part of voice contains obvious non-neutral emotional information in the non-neutral emotional sentence. In other word, some shorter sub-sentences, which belong to a sentence with non-neutral emotion, may not contains any emotional information. Such sub-sentences are used as the inputs for training neural network, which may lead to reduce accuracy of the model. Second, in the predicting stage, the sentence also need to  split into sub-sentences and average the posterior probabilities to get the final result. This would also cause the loss of some accuracy. Our study aims to solve this problem. We first extract the spectrogram for the whole sentence, and then the spectrogram, as the input of deep neural network, is classified to the emotion categories, as shown in Figure~\ref{fig:flow}. For comparison, we use the similar spectrogram extraction setting and neural network as those used in~\cite{lee2011}.

\begin{figure}[!htb]
%\setlength{\abovecaptionskip}{-0.3cm}
%\setlength{\belowcaptionskip}{-0.5cm}
\centering
\centerline{\includegraphics[width=10cm,height=20cm]{flow}}
%  \vspace{2.0cm}
\caption{Flow chart of our emotion recognition system.}
\label{fig:flow}
\end{figure}

\subsection{Spectrogram Extraction}
\label{ssec:spectrogram_extraction}

The speech signal in the IEMOCAP dataset is sampled at 16KHz and organized as single sentences with duration from less than a second to about 20 second. Each sentence is labeled with one emotion. A sequence of overlapping Hamming windows is applied the speech signal, with frame step (window shift) of 10msec, and frame length (window size) of 40 msec. For each frame we calculate a DFT of length 1600 (for 10Hz grid resolution). We use the frequency range of 0-4KHz, ignoring the rest. Following aggregation of the short-time spectra, we obtain a matrix of size NxM, where N according to the variable speech sentence length, and M=400 according to the selected frequency grid resolution.

Next, we implement a normalization step. First, the DFT data is converted to log-power-spectrum, expressed in dB; and then we normalized the energy of frequency domain using z-normalization with mean and standard deviation of training dataset. The normalization allows us to use the spectrogram across multiple different speakers and to eliminate the effect of variations in individual speakers' speaking characteristics.

At last, for improving the computing efficiency, the spectrograms, whose timing length have little difference, need to be putted in the same batch and padded to the max size of spectrograms in the corresponding batch data with zero. Parallel computing can be achieved for a batch of samples during training stage.

\subsection{Deep Neural Network}
\label{ssec:deep_neural_network}

As we all known, convolutional neural networks (CNNs) can be thought of as a kind of neural network that uses many identical copies of the same neuron. This allows the network to have lots of neurons and express computationally large models while keeping the number of actual parameters 每 the values describing how neurons behave 每 that need to be learned fairly small. A lot of works have been carried out using CNNs for feature learning. The common used means, especially in the computer vision, is to process the inputs into the same size. This is convenient to connect other neural network, such as full connected layer. But in fact, the convolutional neural network just need to train a convolutional kernel, so it can be trained, even if the input size is different.

Recurrent Neural Networks (RNNs) are popular models that have shown great promise in many sequence modeling tasks. They perform the same task for every element of a sequence, with the output being depended on the previous computations. For the computing efficiency, the input sequence is usually fixed-length. But we can ignore the outputs of invalid padding time-steps, so that the variable-length sequence can be processed correctly.

In our work, the input sequence are padded into the same length with zero in the same batch, so our neural network need to possess the capacity to avoid the interference of padding value on the output. Let $S=[x_1, x_2, ..., x_V, ..., x_T]$ is a input sequence, where $S1=[x_1, x_2, ..., x_V]$ is the valid part and $S2=[x_{V+1}, x_{V+2}, ..., x_T]$ is the padding part.

First, for the convolution neutral network, we can use masking to reserve the outputs from $S1$, which can be represented as follows:
\begin{equation}
\label{eq:masking}
S_{conv}=Conv(S) \bullet Mask(S)
\end{equation}
where $Conv(S)$ is the output of convolution layer for $S$, $Mask(S)$ is the masking matrix, and $S_{conv}=[y_1, y_2, ..., y_V, ..., y_T]$ is the output sequence with the same length as $S$. The values of masking matrix are ones for the valid part, and others are zeros for the padding part. The valid output can be achieved by element-wise multiply. Besides, convolutional layers are often interweaved with pooling layers. We need to take care of the border value between the valid part and the padding part, which could introduce the invalid information. For example, assuming $S_{conv}$ is the input of the max-pooling layer. The pooling kernel size is 2, so the output of the patch, including $y_V$ and $y_{V+1}$, will be $y_{V+1}$ when $y_V<0$ and $y_{V+1}=0$. But the expected value should be $y_V$ because $y_{V+1}$ is a padding value. This problem may lead to that the neural network does not converge. Hence, the $y_V$ will be masked as zero before inputting to the max-pooling layer in our design.

Second, for the recurrent neural network, because speech emotion recognition is a sequence classification problem, we just need the output in the last valid time-step. Assuming $S$ is the input of recurrent neural network, the expected result should be the output at $t=V$. Besides, in the bi-directional recurrent neural network, the output of backward recurrent neural network should be at $t=0$. The final output is a concatenation of outputs in forward and backward recurrent neural network.

We hypothesized that the convolutional neural networks, capable of learning spatial patterns, will learn effectively spatial spectrogram patterns that represent the emotional information. We also hypothesized that adding an LSTM layer will help learning the temporal behavior across the sentence being represented by the spectrogram. Our deep neural network model is depicted in Figure~\ref{fig:network}.

\begin{figure}[!htb]
%\setlength{\abovecaptionskip}{-0.3cm}
%\setlength{\belowcaptionskip}{-0.5cm}
\centering
\centerline{\includegraphics[width=10cm, height=20cm]{network}}
%\vspace{2.0cm}
\caption{Deep neutral network topology}
\label{fig:network}
\end{figure}

Since the length of sentence is different, we will assign different weights to the loss, in inverse proportion to the length. Besides, the IEMOCAP corpus is significantly unbalanced, the number of some emotional categories is obviously more than other emotional categories. The weight, in inverse proportion to the class size, will also be assigned to the loss.


\section{Experiments}
\label{sec:expeiments}
\subsection{Experimental Setup}
\label{ssec:experimental setup}

In this work, the IEMOCAP dataset~\cite{busso2008} is used for conducting the experiments. The dataset was designed for studying multimodal expressive dyadic interactions. It was collected using motion capture and audio/video recording (approximately a total of 12h) over 5 dyadic sessions with 10 subjects. Each session consists of a different dyad where one male and one female actor perform scripted plays and engage in spontaneous improvised dialogs elicited through affective scenario prompts. At least three evaluators annotated each utterance in the dataset with the categorical emotion labels chosen from the set: happy, sad, neutral, angry, surprised, exited, frustration, disgust, fear and other. We consider only the utterances with majority agreement (at least two out of three evaluators gave the same emotion label) over the emotion classes of: Angry, Happy, Sad and Neutral. The dataset contains scripted and improvised dialogs. As the script text exhibits strong correlation with the labeled emotions, it may give rise to lingual content learning, at least partially, which is an undesired side effect. Therefore we used the improvised data only. Such configuration is the same as~\cite{lee2011}, which makes the experimental results comparable between our work and~\cite{lee2011}.

Our work focuses on speaker independent emotion recognition, hence the 5-folds cross-validation method is used to conduct the experiments. In each fold, the data from four sessions is used for training the deep neural network, and the data from the remaining session is split 每 one speaker for validation and the other for the accuracy testing.

The experimental results can be divided into two parts. In the first part, we show the spectrogram of different speech segment belonging to the same sentence, whose difference can be used to verify the our assumption about only a part of voice contains obvious non-neutral emotional information in the non-neutral emotional sentence. In the second part, we compare and analyze the performance of the variable-length neural network and the fixed-length neural network.

\subsection{Experimental Results}
\label{ssec:experimental results}

It is informative to examine the difference between two speech segments from the same sentence, by looking at spectrograms. The Figure~\ref{fig:spectrogram} below show the spectrograms of two different speech segments (length is 3s), from the same sentence labeled as Angry.

\begin{figure}[htb]
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=5.0cm]{neu}}
%  \vspace{1.5cm}
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=5.0cm]{ang}}
%  \vspace{1.5cm}
\end{minipage}
%
\caption{Left: non-emotional spectrogram; Right: emotional spectrogram}
\label{fig:spectrogram}
\end{figure}

In the Figure~\ref{fig:spectrogram}, the left side 每 shows the non-emotional spectrogram; the right side 每 shows the emotional spectrogram. The horizontal axis denotes the time, and the vertical denotes the frequency. The gray scale represents the energy strength - darker colors designate higher energy. It is clearly seen that the right spectrogram shows higher energy than the left spectrogram in most of frequency range. In general, the loudness of angry voice is higher, so the difference of two spectrograms can reflect that the right speech segment looks more like a angry voice than the left speech segment. Besides, through artificial listening, we also find that the right speech segment contains more intense anger, but the left speech segment is more like a neutral speech segment. If all of two speech segments are labeled as Angry, it may cause confusion between Neutral and Angry in the model training and lead to the actual neutral speech segment is misclassified. However, when we listen to the whole sentence, the front neutral speech segment can enhance angry feeling of the back angry speech segment, the so-called angry ending without angry beginning. Hence, this proves that the whole sentence as the input is more reasonable.

Next, we conduct emotion recognition experiments by reporting the weighed accuracies (WA) and the unweighted accuracy (UA) of different methods, where WA is the accuracy of all samples in the test set and UA is the average value of the accuracy values of all emotions. Both metrics are standard measurements used in several previous emotion recognition challenge and is adopted in~\cite{lee2011}. Also, owing to the parameters of our network is different from \cite{lee2011}, we also show the result about the fixed-length speech segment as the input for our network. This is to ensure that the improvement of performance don't come from parameters changing.

In Table~\ref{tab:accuracy_en}, we present weighted accuracy (WA) and unweighted accuracy (UA) in the IEMOCAP dataset, where ``Baseline'' represents the result reported in~\cite{lee2011}, ``Fixed-Length'' represents the fixed-length inputs for our neural network, ``Variable-Length'' represents the variable-length inputs for our neural network. As can be seen, the UA of ``Variable-Length'' is 62.54\%, which is a 4.08\% absolute (6.98\% relative) improvement over ``Baseline''; And the WA also reaches 57.85\%, which is a 1.47\% absolute (2.61\% relative) improvement compared to ``Baseline''. Besides, the WA and UA of ``Fixed-Length'' is also lower than ``Variable-Length'', so this proves that the improvement of performance don't come from parameters changing.

To further analyze the experimental result, we present the confusion matrices of both the fixed-length input and the variable-length input for our network in Table~\ref{tab:cm_base} and Table~\ref{tab:cm_blr}. We can find that the recognition accuracies of Neutral for our variable-length network are improved. As we have made the analysis for Figure~\ref{fig:spectrogram}, this is because the confusion between Neutral and other emotions can be alleviated by inputting the whole sentence to the network. These results indicate that the variable-length neural network really provides improvement in the recognition accuracy of Neutral.

The accuracy of Neutral does not show great variation, which is probably because the distances between Neutral and other emotions do not differ so much. These results indicate that the prior information of dimensional emotion space really provides helpful information to the decision fusion of the emotion-pair method. Compared to the the hierarchical binary decision tree, our framework can provide improvement in the recognition accuracy of non-neutral emotions.


\begin{table}[!htb]
\caption{Comparison of weighted accuracy (WA) and unweighted accuracy (UA) between the hierarchical binary decision tree (Baseline) and the emotion-pair method (Emotion-pair) on USC IEMOCAP database. }
\scalebox{0.95}{
\begin{tabular}{p{3cm}<{\centering} | p{2cm}<{\centering} | p{2cm}<{\centering}}
    \hline
    & WA & UA \\
    \hline
    Baseline & 56.38\% & 58.46\% \\
    \hline
    Emotion-pair & 57.85\% & 62.54\% \\
    \hline
\end{tabular}}
\label{tab:accuracy_en}
\end{table}


\begin{table}[!htb]
%\setlength{\abovecaptionskip}{-0.1cm}
\setlength{\belowcaptionskip}{-0.5cm}
\caption{Confusion matrix by using the hierarchical binary decision tree with Bayesian Logistic Regression (BLR).}
\scalebox{0.9}{
\begin{tabular}{c|c|c|c|c}
    \hline
        \backslashbox{Actual}{Predict} & Neutral & Angry & Happy & Sad \\
    \hline
        Neutral & \textbf{54.51\%} & 6.89\% & 15.20\% & 23.40\% \\
    \hline
        Angry & 16.62\% & \textbf{65.40\%} & 15.26\% & 2.72\% \\
    \hline
        Happy & 26.13\% & 19.57\% & \textbf{41.72\%} & 12.58\% \\
    \hline
        Sad & 21.70\% & 2.22\% & 3.88\% & \textbf{72.21\%} \\
    \hline
\end{tabular}}
\footnotesize
\label{tab:cm_base}
\end{table}

\begin{table}[!htb]
%\setlength{\abovecaptionskip}{-0.1cm}
%\setlength{\belowcaptionskip}{-1cm}
\caption{Confusion matrix by using the emotion-pair method with Bayesian Logistic Regression (BLR).}
\scalebox{0.9}{
\begin{tabular}{c|c|c|c|c}
    \hline
        \backslashbox{Actual}{Predict} & Neutral & Angry & Happy & Sad \\
    \hline
        Neutral & \textbf{53.98\%} & 6.19\% & 12.39\% & 27.43\% \\
    \hline
        Angry & 15.46\% & \textbf{68.04\%} & 12.37\% & 4.12\% \\
    \hline
        Happy & 21.94\% & 15.82\% & \textbf{50.51\%} & 11.73\% \\
    \hline
        Sad & 14.93\% & 1.49\% & 5.97\% & \textbf{77.61\%} \\
    \hline
\end{tabular}}
\footnotesize
\label{tab:cm_blr}
\end{table}


\section{Conclusion}
\label{sec:conclusion}

In this paper, we propose a variable-length neutral network that operates on the spectrogram, to perform an end-to-end emotion classification task from variable-length speech segments. Further, we analyze the possible influences of splitting each sentence to shorter sub-sentences of equal lengths. Our method can effectively alleviate the confusion between Neutral and other emotions. The weighed accuracies (WA) and the unweighted accuracy (UA) achieves beyond the state of the art accuracy on the common benchmarking dataset IEMOACP, comparing with the previous work of fixed-length neural network. 


and use the Naive Bayes classifier to make the final decision by considering the relationship between different emotions in the dimensional emotion space. Experimental results have proved that our approach can achieve better results compared to the  hierarchical binary decision tree method. Furthermore, our framework can be fully automatically generated without empirical guidance and is easier to be parallelized. Considering the promotion space is relatively large in feature selection, emotion bi-classification and decision fusion, our future work will be devoted to optimize these three parts.

%\section{Acknowledgement}
%\label{sec:acknowledgement}
%
%This work is supported by National High Technology Research and Development Program of China (2015AA016305), National Natural Science Foundation of China (NSFC) (61375027, 61433018, 61370023 and 61171116), joint fund of NSFC-RGC (Research Grant Council of Hong Kong) (61531166002, N CUHK404/15) and Major Program for National Social Science Foundation of China (13\&ZD189).


\bibliographystyle{IEEEtran}

\bibliography{refs}

% \begin{thebibliography}{9}
% \bibitem[1]{Davis80-COP}
%   S.\ B.\ Davis and P.\ Mermelstein,
%   ``Comparison of parametric representation for monosyllabic word recognition in continuously spoken sentences,''
%   \textit{IEEE Transactions on Acoustics, Speech and Signal Processing}, vol.~28, no.~4, pp.~357--366, 1980.
% \bibitem[2]{Rabiner89-ATO}
%   L.\ R.\ Rabiner,
%   ``A tutorial on hidden Markov models and selected applications in speech recognition,''
%   \textit{Proceedings of the IEEE}, vol.~77, no.~2, pp.~257-286, 1989.
% \bibitem[3]{Hastie09-TEO}
%   T.\ Hastie, R.\ Tibshirani, and J.\ Friedman,
%   \textit{The Elements of Statistical Learning -- Data Mining, Inference, and Prediction}.
%   New York: Springer, 2009.
% \bibitem[4]{YourName17-XXX}
%   F.\ Lastname1, F.\ Lastname2, and F.\ Lastname3,
%   ``Title of your INTERSPEECH 2017 publication,''
%   in \textit{Interspeech 2017 -- 18\textsuperscript{th} Annual Conference of the International Speech Communication Association, August 20?24, Stockholm, Sweden, Proceedings, Proceedings}, 2017, pp.~100--104.
% \end{thebibliography}

\end{document}
